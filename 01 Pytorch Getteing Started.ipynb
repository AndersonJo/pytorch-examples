{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Neural Network Model\n",
    "\n",
    "nn.Module 을 사용하여 Neural Network 모델을 만들고, 반드시 forward(input)함수를 만들어줘야 합니다.<br>\n",
    "backward함수는 자동으로 만들어집니다.\n",
    "\n",
    "> torch.nn 전체 모듈 통들어서.. 오직 minibatch만 제공됩니다.<br>\n",
    "> 즉 single sample은 전혀 지원되지 않습니다.<br>\n",
    "> 예를 들어서 4D Tensor는 다음과 같이 사용되어야 합니다.<br><br>\n",
    "> <span style=\"color:#cc4444; font-weight:bold;\">nSamples x nChannels x Height x Width</span><br><br>\n",
    "> 만약 single sample을 사용하려고 한다면 **input.unsqueeze(0)** 를 사용해서 fake batch를 만들어 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear (400 -> 120)\n",
      "  (fc2): Linear (120 -> 84)\n",
      "  (fc3): Linear (84 -> 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5).cuda()\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5).cuda()\n",
    "        self.fc1   = nn.Linear(16 * 5 * 5, 120).cuda()  # an affine operation: y = Wx + b\n",
    "        self.fc2   = nn.Linear(120, 84).cuda()\n",
    "        self.fc3   = nn.Linear(84, 10).cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))  # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)  # If the size is a square you can only specify a single number\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Predict\n",
    "\n",
    "GPU에서 처리한 값을 CPU로 불러와서 처리해야 하기 때문에 .cpu 함수를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04526739,  0.01011707,  0.08764283, -0.07252117,  0.08807731,\n",
       "         0.00853545, -0.01395075,  0.0785292 , -0.06747445,  0.0142101 ]], dtype=float32)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = Variable(torch.randn(1, 1, 32, 32)).cuda()\n",
    "output = net(input)\n",
    "\n",
    "output.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 28.4200\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target = Variable(torch.arange(0, 10)).cuda()\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.nn._functions.thnn.auto.MSELoss object at 0x7f2bc823f588>\n",
      "<torch.nn._functions.linear.Linear object at 0x7f2bc8252668>\n",
      "<torch.nn._functions.thnn.auto.Threshold object at 0x7f2bc8252ac8>\n"
     ]
    }
   ],
   "source": [
    "print(loss.creator)  # MSELoss\n",
    "print(loss.creator.previous_functions[0][0])  # Linear\n",
    "print(loss.creator.previous_functions[0][0].previous_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Backpropagation\n",
    "\n",
    "backpropagate the error 하기전에 반드시 .zero_grad()를 사용해서 gradients값을 0으로 만들어줘야 합니다. <br>\n",
    "이렇게 하지 않고, .backward()를 사용시 gradients값들이 accumulate되게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "Variable containing:\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
      "\n",
      "conv1.bias.grad after backward\n",
      "Variable containing:\n",
      "1.00000e-02 *\n",
      "  0.0102\n",
      " -5.8490\n",
      "  4.7722\n",
      "  1.1550\n",
      " -2.7535\n",
      "  0.2246\n",
      "[torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()   \n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Update the weights\n",
    "\n",
    "가장 간단한 Stochastic Gradient Descent에 따르면 update는 다음과 같습니다.\n",
    "\n",
    "$$ \\text{weight} = \\text{weight} - \\text{learning rate} * \\text{gradient} $$\n",
    "\n",
    "파이썬에서 간단하게 하려면 다음과 같이 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "하지만 SGD, Nesterov-SGD, Adam, RMSProp 같은 다양한 방법으로의 update가 존재합니다.<br>\n",
    "torch.optim 모듈은 이미 다양한 optimizers들을 제공하고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# In your training loop\n",
    "# loop 안에서.. \n",
    "optimizer.zero_grad()\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step() # update를 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Learnable Parameters\n",
    "\n",
    "Learnable Parameters는 parameters() 함수를 통해서 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 5, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([16, 6, 5, 5])\n",
      "torch.Size([16])\n",
      "torch.Size([120, 400])\n",
      "torch.Size([120])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([84])\n",
      "torch.Size([10, 84])\n",
      "torch.Size([10])\n",
      "\n",
      "Parameter containing:\n",
      " 3.6837e-02 -2.7158e-02 -2.4049e-02  ...   2.8363e-02  2.7047e-02 -2.0186e-02\n",
      " 2.6292e-02  2.1365e-04  4.3966e-02  ...   1.8812e-02 -3.9648e-02 -3.7087e-02\n",
      "-3.0545e-02 -2.0370e-02 -4.4853e-03  ...  -1.6517e-02 -4.8652e-02 -6.4337e-03\n",
      "                ...                   ⋱                   ...                \n",
      "-2.9419e-02 -2.1754e-03 -1.3552e-02  ...  -1.5166e-02  2.2346e-03  3.7931e-02\n",
      " 3.8603e-02 -3.4742e-03  2.1454e-02  ...  -1.6677e-02 -4.9337e-03  9.5037e-03\n",
      " 1.9295e-02  4.7208e-02  1.8468e-02  ...  -1.0812e-02  5.0891e-03  2.5843e-02\n",
      "[torch.cuda.FloatTensor of size 120x400 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "for param in params:\n",
    "    print(param.size())\n",
    "\n",
    "print()\n",
    "print(params[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Gradients값 random으로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad Variable containing:\n",
      "-0.0530\n",
      "-0.0365\n",
      "-0.0691\n",
      "-0.1326\n",
      " 0.0075\n",
      " 0.0064\n",
      "[torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
      "\n",
      "data \n",
      "-0.0331\n",
      "-0.1378\n",
      " 0.1596\n",
      "-0.0864\n",
      " 0.0523\n",
      " 0.1236\n",
      "[torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.backward(torch.randn(1, 10).cuda())\n",
    "params = list(net.parameters())\n",
    "print('grad', params[1].grad)\n",
    "print('data', params[1].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Gradients값 zero로 만들기\n",
    "\n",
    ".zero_grad를 사용하게 되면 gradients값을 zero로 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad Variable containing:\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
      "\n",
      "data \n",
      "-0.0331\n",
      "-0.1378\n",
      " 0.1596\n",
      "-0.0864\n",
      " 0.0523\n",
      " 0.1236\n",
      "[torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "params = list(net.parameters())\n",
    "print('grad', params[1].grad)\n",
    "print('data', params[1].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.0331\n",
       "-0.1378\n",
       " 0.1596\n",
       "-0.0864\n",
       " 0.0523\n",
       " 0.1236\n",
       "[torch.cuda.FloatTensor of size 6 (GPU 0)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = params[1]\n",
    "a.data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
