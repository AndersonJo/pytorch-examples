{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning\n",
    "\n",
    "* [Deep Q Learning Nature Paper - Human-level control through deep reinforcement\n",
    "learning](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)\n",
    "* [Target Network에 대한 자세한 Paper - CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING](https://arxiv.org/pdf/1509.02971.pdf)\n",
    "* [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)\n",
    "\n",
    "\n",
    "**Temporal Difference Learning**\n",
    "\n",
    "* [An Introduction to Temporal Difference Learning](http://www.ias.informatik.tu-darmstadt.de/uploads/Teaching/AutonomousLearningSystems/Kunz_ALS_2013.pdf)\n",
    "* [Temporal-Difference Learning](https://www.tu-chemnitz.de/informatik/KI/scripts/ws0910/ml09_6.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Deep Q Network의 특징 \n",
    "\n",
    "Q-network에서 neural network를 사용하면 DQN은 다음과 같은 특징을 통해 향상을 시켰습니다. \n",
    "\n",
    "1. Multi-layer convolutional network를 통해서 더 복잡한 문제를 해결함 \n",
    "2. Experience Replay를 통해 더 stable한 모델을 만들수 있게 됨\n",
    "3. target network 를 사용 (즉 2개의 neural network를 사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Discounted Return \n",
    "\n",
    "Agent의 목표는 future reward를 최대치로 하는 actions을 선택하는 것입니다.<br>\n",
    "여기서 future reward란 $ \\gamma $ 배수만큼 (a factor of $ \\gamma $ per time-step) discounted 되는 것을 의미합니다. <br>\n",
    "쉽게 이야기해서 먼미래의 reward일수록, 더 적은 reward로 계산하겠다는 뜻입니다.\n",
    "\n",
    "### $$ R_t = \\sum^T_{t^{\\prime} = t} \\gamma^{t^{\\prime} - t} r_{t^{\\prime}} $$\n",
    "\n",
    "$ T $는 게임이 끝나는 시점의 time-step을 의미하며 $ \\gamma \\in [0, 1] $ 의 값을 갖습니다.\n",
    "\n",
    "아래의 코드에서는 gamma 값이 시간의 흐름에 따른 변화를 그렸습니다.<br>\n",
    "먼 미래의 reward값일수록 0에 가까운 gamma값과 곱해야 하기 때문에<br>\n",
    "코앞의 reward는 가중치가 높고, 먼 미래는 reward는 가중치가 낮게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f288bec50f0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f288bf3cdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def gamma_values(gamma=0.9, n=50):\n",
    "    return [gamma**i for i in range(1, n)]\n",
    "\n",
    "plot(gamma_values())\n",
    "title('gamma value in regard with t. gamma=0.9, n=50')\n",
    "xlabel('time t')\n",
    "ylabel('gamma value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal action-value Function and Bellman Equation \n",
    "\n",
    "기본적으로 Reinforcement Learning의 목표는 expected return을 최대치로 하는 policy를 학습하는 것이며,<br>\n",
    "아래의 공식과 같은 <span style=\"color:red\">**Optimal action-value function $ Q^{*}(s, a) $**</span> 을 사용합니다. <br>\n",
    "즉 어떤 특정 state $ s_t $ 에서 특정 action $ a_t $ 를 취했을때 얻게되는 expected return 을 나타냅니다.\n",
    "\n",
    "$$ \\begin{align}\n",
    "Q^{*} (s, a) &= \\max_{\\pi} \\mathbb{E} \\left[ r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + ... \\big|\\ s_t =s, a_t = a, \\pi \\right] \\\\\n",
    "&= \\max_{\\pi} \\mathbb{E} \\left[ R_t \\big| \\ s_t=s, a_t =a, \\pi \\right] \n",
    "\\end{align} $$\n",
    "\n",
    "Optimal Action-Value Function은  <span style=\"color:red\">**Bellman Equation**</span>을 따릅니다. <br>\n",
    "만약 optimal value $ Q^{*}(s^{\\prime}, a^{\\prime}) $ (여기서 $ s^{\\prime} $는 next time-step의 state이고, $ a^{\\prime} $는 모든 가능한 actions들을 말함) 의 값을 알고 있다면, <br>\n",
    "expected value $ r + \\gamma Q^{*}(s^{\\prime}, a^{\\prime}) $를 maximize 하는 action $ a^{\\prime} $을 선택하는 것에 기초를 두고 있습니다.\n",
    "\n",
    "$$ Q^{*} (s, a) = E_{s^{\\prime} \\sim \\varepsilon} \\left[ r + \\gamma \\max_{a^{\\prime}} Q^{*} \\left(s^{\\prime}, a^{\\prime}\\right) \\  \\big| \\ s, a \\right] $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear Approximator and Loss Function\n",
    "\n",
    "기본적으로 많은 reinforcement learning algorithms들의 아이디어는 <br>\n",
    "위의 Bellman Equation을 사용하여 iterative update로서 action-value function을 구하는 것입니다.<br>\n",
    "다음은 Value Iteration같은 알고리즘같이 **iterative update**를 사용하는 공식. \n",
    "\n",
    "$$ Q_{i+1}(s, a) = \n",
    "\\mathbb{E}_{s^{\\prime}} \\left[ r + \\gamma \\max_{a^{\\prime}} Q^{*} \\left( s^{\\prime}, a^{\\prime} \\right) \\big| \\ s, a \\right] $$\n",
    "\n",
    "하지만 위의 공식은 각각의 sequence마다 action-value가 측정되며 일반화시키지 못합니다. <br>\n",
    "따라서 **function approximator**를 사용하여 action-value function을 측정합니다.\n",
    "\n",
    "$$ Q(s, a; \\theta) \\approx Q^{*} (s, a) $$\n",
    "\n",
    "보통 Reinforcement Learning에서는 Linear function approximator를 사용하지만, <br>\n",
    "Deep Q Learning에서는 Nonlinear function approximator인 뉴럴네트워크를 사용합니다.<br>\n",
    "위의 공식에서 neural network function approximator로서 weights $ \\theta $가 Q-network로 사용되었습니다.<br>\n",
    "즉 Q-network는 parameters $ \\theta $를 조정해가면서 학습이 진행됩니다.\n",
    "\n",
    "Loss function은 mean-squared error를 사용합니다.<br>\n",
    "이때 Bellman equation을 사용하게 되는데 target values $ r + \\gamma \\max_{a^{\\prime}} Q^{*}\\left(s^{\\prime}, a^{\\prime}\\right) $ 이 부분을 <br>\n",
    "approximate target values $ y = r + \\gamma \\max_{a^{\\prime}} Q \\left( s^{\\prime}, a^{\\prime}; \\theta_{i-1} \\right) $ 으로 대체시켜줍니다. \n",
    "\n",
    "최종적으로 <span style=\"color:red\">**Loss function**</span>은 다음과 같습니다. \n",
    "\n",
    "\n",
    "$$ L_i (\\theta_i) = \\mathbb{E}_{s, a \\sim p(\\cdot)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experience Replay\n",
    "\n",
    "학습시 Replay Memory에서 저장된 샘플 $ (s, a, r, s^{\\prime}) \\sim U(D) $  을 사용합니다.<br>\n",
    "replay memory를 사용안하면 environment (Atari Game) 에서 현재 진행되고 있는 states들로만 학습이 이루어지며, 이는 왜곡된 학습을 할 수 있습니다. <br>\n",
    "Replay Memory는 과거의 다양한 경험들을 사용해서 학습을 할 수 있도록 해줍니다.\n",
    "\n",
    "\n",
    "#### Seperate Target Network \n",
    "\n",
    "RLNeural Network를 사용함에 \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Experience Replay and Loss Function\n",
    "\n",
    "게임을 진행하면서 학습을 할 경우 observation sequence같의 연관성(correlation)때문에 학습이 제대로 안 될수 있습니다.<br>\n",
    "연관성을 끊어주는 방법으로 experience replay를 사용합니다.\n",
    "\n",
    "먼저 Agent의 experiences $ e_t = (s_t, a_t, r_t, s_{t+1} )$를 각각의 time-step마다 data set $ D_t = \\{ e_1, e_2, ..., e_t \\} $에 저장합니다.<br>\n",
    "학습시 Q-Learning updates를 uniformly random으로 꺼내진 experiences $ (s, a, r, s^{\\prime}) \\sim U(D) $ 통해 실행하게 됩니다.<br>\n",
    "Q-Learning update는 다음의 loss function을 사용하게 됩니다.\n",
    "\n",
    "$$ L_i(\\theta_i) = \\mathbb{E}_{s, a, r, s^{\\prime}} \\sim U(D) \\left[ \\left( r + \\gamma \\max_{a^{\\prime}} Q\\left(s^{\\prime}, a^{\\prime}; \\theta_{i-1}\\right) - Q\\left( s, a; \\theta_i \\right) \\right)^2 \\right] $$\n",
    "\n",
    "위의 공식을 Differentiate하면 다음과 같은 결과를 얻습니다.\n",
    "\n",
    "$$ \\nabla $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\nabla_{\\theta_i} L_i (\\theta_i) = \\mathbb{E}_ $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Learning with Experience Replay Algorithm\n",
    "\n",
    "<img src=\"./images/deep-q-learning-algorithm.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDPG Algorithm\n",
    "\n",
    "Deep Deterministic policy gradient를 뜻하며 target network의 알고리즘을 포함하고 있습니다.\n",
    "\n",
    "<img src=\"./images/ddpg.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms as T\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay\n",
    "\n",
    "Nature에 실린 paper에 따르면.. Experience Replay 또는 Memory Replay라고 하며,<br>\n",
    "각 time-step마다 Agent's experiences를 다음과 같이 저장을 합니다. \n",
    "\n",
    "$$ \\begin{align}\n",
    "e_t &= (s_t, a_t, r_t, s_{t+1}) \\\\\n",
    "D_t &= \\{e_1, ..., e_t\\}\n",
    "\\end{align} $$\n",
    "\n",
    "여러개의 episodes의 experiences를 갖고 있습니다. <br>\n",
    "episode란 예를 들어 게임 한 번을 실행하여 terminal state에 도달하게 된 것을 의미하는데, <br>\n",
    "여러번의 게임 경험을 experience replay에서 갖고 있다는 뜻입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, size=10000):\n",
    "        self.size = size\n",
    "        \n",
    "replay = ReplayMemory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Network\n",
    "\n",
    "Input으로는 전처리 $ \\phi $를 거친  84 x 84 x 4 images를 받으며 deep convolutional neural network를 사용합니다.<br>\n",
    "output의 갯수는 actions의 갯수가 되어야 하며, 이미지의 위치를 유지하기 위하여 pooling은 사용하지 않습니다.<br>\n",
    "(예를 들어, 막대기 세우기 게임에서는 $ Q(s, \\text{left}) $ 그리고 $ Q(s, \\text{right}) $ 가 되어야 합니다.\n",
    "\n",
    "> nn.Conv2d는 $ (N, C_{in}, H, W) $ 의 형태로 이미지를 받아야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_action):\n",
    "        super(DQN, self).__init__()\n",
    "        self.n_action = n_action\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=20, stride=2) # (In Channel, Out Channel, ...)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=9, stride=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=5, stride=2)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.affine1 = nn.Linear(512, self.n_action)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = F.leaky_relu(self.bn1(self.conv1(x)))\n",
    "        h = F.leaky_relu(self.bn2(self.conv2(h)))\n",
    "        h = F.leaky_relu(self.bn3(self.conv3(h)))\n",
    "        out = self.affine1(h.view(h.size(0), -1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-10 23:28:04,824] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: (4,)\n",
      "action space: 2\n",
      "screen (400, 600, 3)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "print('observation space:', env.observation_space.shape)\n",
    "print('action space:', env.action_space.n)\n",
    "screen = env.render(mode='rgb_array')\n",
    "print('screen', screen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "env.step(1)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ..., \n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ..., \n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ..., \n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       ..., \n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ..., \n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ..., \n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ..., \n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]]], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-12 09:45:44,739] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "GAME_NAME = 'CartPole-v0'\n",
    "\n",
    "class Environment(object):\n",
    "    def __init__(self, game, width=84, height=84):\n",
    "        self.game = gym.make(game)\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self._toTensor = T.Compose([T.ToPILImage(), T.ToTensor()])\n",
    "    \n",
    "    def play_sample(self):\n",
    "        observation = self.game.reset()\n",
    "        while True:\n",
    "            screen = self.game.render(mode='rgb_array')\n",
    "            screen = self.preprocess(screen)\n",
    "            action = self.game.action_space.sample()\n",
    "            observation, reward, done, info = self.game.step(action)\n",
    "            if done:\n",
    "                break\n",
    "        self.game.close()\n",
    "        \n",
    "    def preprocess(self, screen):\n",
    "        preprocessed = cv2.resize(screen, (self.height, self.width)) # 84 * 84 로 변경\n",
    "        preprocessed = preprocessed.transpose((2, 0, 1)) # (C, W, H) 로 변경\n",
    "        return preprocessed\n",
    "    \n",
    "    def init(self):\n",
    "        \"\"\"\n",
    "        @return observation\n",
    "        \"\"\"\n",
    "        return self.game.reset()\n",
    "    \n",
    "    def get_screen(self):\n",
    "        screen = self.game.render(mode='rgb_array')\n",
    "        screen = self.preprocess(screen)\n",
    "        return screen\n",
    "    \n",
    "    def toVariable(self, x):\n",
    "        return Variable(self._toTensor(x).cuda())\n",
    "    \n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return self.game.action_space.n\n",
    "    \n",
    "    \n",
    "    \n",
    "env = Environment(GAME_NAME)\n",
    "env.play_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-12 09:51:04,044] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "INITIAL_EPSILON = 1.0\n",
    "FINAL_EPSILON = 0.05\n",
    "EXPLORATION_STEPS = 1000000\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, cuda=True):\n",
    "        # Environment\n",
    "        self.env = Environment(GAME_NAME)\n",
    "        \n",
    "        # DQN Model\n",
    "        self.dqn = DQN(self.env.action_space)\n",
    "        if cuda:\n",
    "            self.dqn.cuda()\n",
    "        self.optimizer = optim.RMSprop(self.dqn.parameters(), lr=0.0025)\n",
    "        \n",
    "        # Replay Memory \n",
    "        self.replay_momory = ReplayMemory()\n",
    "        \n",
    "        # Epsilon\n",
    "        self.epsilon = INITIAL_EPSILON\n",
    "        self.epsilon_step = (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORATION_STEPS # 9.499999999999999e-07\n",
    "    \n",
    "    def train(self):\n",
    "        pass\n",
    "        \n",
    "        \n",
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
