{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Deep Reinforcement Learning\n",
    "\n",
    "* [Deep Q Learning Nature Paper - Human-level control through deep reinforcement\n",
    "learning](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)\n",
    "* [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)\n",
    "\n",
    "\n",
    "## Agent and Environment\n",
    "\n",
    "Agent는 Environment(여기서는 Atari Game)과 연동을 합니다.<br>\n",
    "각각의 time-step마다 Agent는 $ A = {1, ..., K} $중의 하나의 액션 $ a_t $를 선택하고, Environment로부터 화면 이미지 $ x_t \\in \\mathbb{R}^d $와 reward $ r_t $를 받습니다.<br>\n",
    "하지만 하나의 게임 화면 이미지로만으로는 예를 들어 블록깨기에서 공이 어디 방향으로 가는지 알 수가 없습니다. <br>\n",
    "따라서 $ x_t $를 시계열의 데이터로 받으며 일련의 actions과 observations 의 연속성이 됩니다.\n",
    "\n",
    "$$ s_t = x_1, a_1, x_2, ..., a_{t-1}, x_t  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Future Discounted Return \n",
    "\n",
    "Agent의 목표는 future reward를 최대치로 하는 actions을 선택하는 것입니다.<br>\n",
    "여기서 future reward란 $ \\gamma $ 배수만큼 (a factor of $ \\gamma $ per time-step) discounted 되는 것을 의미합니다. <br>\n",
    "쉽게 이야기해서 먼미래의 reward일수록, 더 적은 reward로 계산하겠다는 뜻입니다.\n",
    "\n",
    "### $$ R_t = \\sum^T_{t^{\\prime} = t} \\gamma^{t^{\\prime} - t} r_{t^{\\prime}} $$\n",
    "\n",
    "$ T $는 게임이 끝나는 시점의 time-step을 의미합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Optimal action-value Function and Bellman Equation \n",
    "\n",
    "<span style=\"color:red\">**Optimal action-value function $ Q^{*}(s, a) $**</span>이란 \n",
    "policy를 따름으로서 얻을수 있는 maximum expected return으로 정의할수 있으며 다음의 공식과 같습니다.<br>\n",
    "(policy $ \\pi $는  state -> action 으로 연결시키는 매핑이라고 쉽게 생각할 수 있습니다. 다르게 말하면 distributions over actions)\n",
    "\n",
    "$$ \\begin{align}\n",
    "Q^{*} (s, a) &= \\max_{\\pi} \\mathbb{E} \\left[ r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + ... \\big|\\ s_t =s, a_t = a, \\pi \\right] \\\\\n",
    "&= \\max_{\\pi} \\mathbb{E} \\left[ R_t \\big| \\ s_t=s, a_t =a, \\pi \\right] \n",
    "\\end{align} $$\n",
    "\n",
    "Optimal Action-Value Function은 <span style=\"color:red\">**Bellman Equation**</span>을 따릅니다. <br>\n",
    "만약 optimal value $ Q^{*}(s^{\\prime}, a^{\\prime}) $ (여기서 $ s^{\\prime} $는 next time-step의 state이고, $ a^{\\prime} $는 모든 가능한 actions들을 말함) 의 값을 알고 있다면, <br>\n",
    "expected value $ r + \\gamma Q^{*}(s^{\\prime}, a^{\\prime}) $를 maximize 하는 action $ a^{\\prime} $을 선택하는 것에 기초를 두고 있습니다.\n",
    "\n",
    "$$ Q^{*} (s, a) = E_{s^{\\prime} \\sim \\varepsilon} \\left[ r + \\gamma \\max_{a^{\\prime}} Q^{*} \\left(s^{\\prime}, a^{\\prime}\\right) \\  \\big| \\ s, a \\right] $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Nonlinear Approximator and Loss Function\n",
    "\n",
    "기본적으로 많은 reinforcement learning algorithms들의 아이디어는 <br>\n",
    "위의 Bellman Equation을 사용하여 iterative update로서 action-value function을 구하는 것입니다.<br>\n",
    "다음은 Value Iteration같은 알고리즘같이 **iterative update**를 사용하는 공식. \n",
    "\n",
    "$$ Q_{i+1}(s, a) = \n",
    "\\mathbb{E}_{s^{\\prime}} \\left[ r + \\gamma \\max_{a^{\\prime}} Q^{*} \\left( s^{\\prime}, a^{\\prime} \\right) \\big| \\ s, a \\right] $$\n",
    "\n",
    "하지만 위의 공식은 각각의 sequence마다 action-value가 측정되며 일반화시키지 못합니다. <br>\n",
    "따라서 **function approximator**를 사용하여 action-value function을 측정합니다.\n",
    "\n",
    "$$ Q(s, a; \\theta) \\approx Q^{*} (s, a) $$\n",
    "\n",
    "보통 Reinforcement Learning에서는 Linear function approximator를 사용하지만, <br>\n",
    "Deep Q Learning에서는 Nonlinear function approximator인 뉴럴네트워크를 사용합니다.<br>\n",
    "위의 공식에서 neural network function approximator로서 weights $ \\theta $가 Q-network로 사용되었습니다.<br>\n",
    "즉 Q-network는 parameters $ \\theta $를 조정해가면서 학습이 진행됩니다.\n",
    "\n",
    "Loss function은 mean-squared error를 사용합니다.<br>\n",
    "이때 Bellman equation을 사용하게 되는데 target values $ r + \\gamma \\max_{a^{\\prime}} Q^{*}\\left(s^{\\prime}, a^{\\prime}\\right) $ 이 부분을 <br>\n",
    "approximate target values $ y = r + \\gamma \\max_{a^{\\prime}} Q \\left( s^{\\prime}, a^{\\prime}; \\theta_{i-1} \\right) $ 으로 대체시켜줍니다. \n",
    "\n",
    "최종적으로 <span style=\"color:red\">**Loss function**</span>은 다음과 같습니다. \n",
    "\n",
    "\n",
    "$$ L_i (\\theta_i) = \\mathbb{E}_{s, a \\sim p(\\cdot)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Experience Replay and Loss Function\n",
    "\n",
    "게임을 진행하면서 학습을 할 경우 observation sequence같의 연관성(correlation)때문에 학습이 제대로 안 될수 있습니다.<br>\n",
    "연관성을 끊어주는 방법으로 experience replay를 사용합니다.\n",
    "\n",
    "먼저 Agent의 experiences $ e_t = (s_t, a_t, r_t, s_{t+1} )$를 각각의 time-step마다 data set $ D_t = \\{ e_1, e_2, ..., e_t \\} $에 저장합니다.<br>\n",
    "학습시 Q-Learning updates를 uniformly random으로 꺼내진 experiences $ (s, a, r, s^{\\prime}) \\sim U(D) $ 통해 실행하게 됩니다.<br>\n",
    "Q-Learning update는 다음의 loss function을 사용하게 됩니다.\n",
    "\n",
    "$$ L_i(\\theta_i) = \\mathbb{E}_{s, a, r, s^{\\prime}} \\sim U(D) \\left[ \\left( r + \\gamma \\max_{a^{\\prime}} Q\\left(s^{\\prime}, a^{\\prime}; \\theta_{i-1}\\right) - Q\\left( s, a; \\theta_i \\right) \\right)^2 \\right] $$\n",
    "\n",
    "위의 공식을 Differentiate하면 다음과 같은 결과를 얻습니다.\n",
    "\n",
    "$$ \\nabla $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "$$ \\nabla_{\\theta_i} L_i (\\theta_i) = \\mathbb{E}_ $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Deep Q-Learning with Experience Replay Algorithm\n",
    "\n",
    "<img src=\"./images/deep-q-learning-algorithm.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms as T\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Experience Replay\n",
    "\n",
    "Nature에 실린 paper에 따르면.. Experience Replay 또는 Memory Replay라고 하며,<br>\n",
    "각 time-step마다 Agent's experiences를 다음과 같이 저장을 합니다. \n",
    "\n",
    "$$ \\begin{align}\n",
    "e_t &= (s_t, a_t, r_t, s_{t+1}) \\\\\n",
    "D_t &= \\{e_1, ..., e_t\\}\n",
    "\\end{align} $$\n",
    "\n",
    "여러개의 episodes의 experiences를 갖고 있습니다. <br>\n",
    "episode란 예를 들어 게임 한 번을 실행하여 terminal state에 도달하게 된 것을 의미하는데, <br>\n",
    "여러번의 게임 경험을 experience replay에서 갖고 있다는 뜻입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, size=10000):\n",
    "        self.size = size\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q-Network\n",
    "\n",
    "Input으로는 전처리 $ \\phi $를 거친  84 x 84 x 4 images를 받으며 deep convolutional neural network를 사용합니다.<br>\n",
    "output의 갯수는 actions의 갯수가 되어야 하며, 이미지의 위치를 유지하기 위하여 pooling은 사용하지 않습니다.<br>\n",
    "(예를 들어, 막대기 세우기 게임에서는 $ Q(s, \\text{left}) $ 그리고 $ Q(s, \\text{right}) $ 가 되어야 합니다.\n",
    "\n",
    "> nn.Conv2d는 $ (N, C_{in}, H, W) $ 의 형태로 이미지를 받아야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=20, stride=2) # (In Channel, Out Channel, ...)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=9, stride=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=5, stride=2)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.affine1 = nn.Linear(512, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = F.leaky_relu(self.bn1(self.conv1(x)))\n",
    "        h = F.leaky_relu(self.bn2(self.conv2(h)))\n",
    "        h = F.leaky_relu(self.bn3(self.conv3(h)))\n",
    "        out = self.affine1(h.view(h.size(0), -1))\n",
    "        return out\n",
    "\n",
    "dqn = DQN()\n",
    "dqn.cuda()\n",
    "optimizer = optim.RMSprop(dqn.parameters(), lr=0.0025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-10 17:59:06,717] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: (4,)\n",
      "action space: 2\n",
      "screen (400, 600, 3)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "print('observation space:', env.observation_space.shape)\n",
    "print('action space:', env.action_space.n)\n",
    "screen = env.render(mode='rgb_array')\n",
    "print('screen', screen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.step(1)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ..., \n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ..., \n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ..., \n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       ..., \n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ..., \n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ..., \n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ..., \n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]]], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-10 17:59:07,503] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "GAME_NAME = 'CartPole-v0'\n",
    "\n",
    "class Environment(object):\n",
    "    def __init__(self, game, width=84, height=84):\n",
    "        self.game = gym.make(game)\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self._toTensor = T.Compose([T.ToPILImage(), T.ToTensor()])\n",
    "    \n",
    "    def play_sample(self):\n",
    "        observation = self.game.reset()\n",
    "        while True:\n",
    "            screen = self.game.render(mode='rgb_array')\n",
    "            screen = self.preprocess(screen)\n",
    "            action = self.game.action_space.sample()\n",
    "            observation, reward, done, info = self.game.step(action)\n",
    "            if done:\n",
    "                break\n",
    "        self.game.close()\n",
    "        \n",
    "    def preprocess(self, screen):\n",
    "        preprocessed = cv2.resize(screen, (self.height, self.width)) # 84 * 84 로 변경\n",
    "        preprocessed = preprocessed.transpose((2, 0, 1)) # (C, W, H) 로 변경\n",
    "        return preprocessed\n",
    "    \n",
    "    def init(self):\n",
    "        \"\"\"\n",
    "        @return observation\n",
    "        \"\"\"\n",
    "        return self.game.reset()\n",
    "    \n",
    "    def get_screen(self):\n",
    "        screen = self.game.render(mode='rgb_array')\n",
    "        screen = self.preprocess(screen)\n",
    "        return screen\n",
    "    \n",
    "    def toVariable(self, x):\n",
    "        return Variable(self._toTensor(x).cuda())\n",
    "    \n",
    "    \n",
    "env = Environment(GAME_NAME)\n",
    "# env.play_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.init()\n",
    "screen = env.get_screen()\n",
    "screen = env.toVariable(screen)\n",
    "\n",
    "env.step()\n",
    "\n",
    "env.game.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
