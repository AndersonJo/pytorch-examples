{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning\n",
    "\n",
    "* [Deep Q Learning Nature Paper - Human-level control through deep reinforcement\n",
    "learning](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)\n",
    "* [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)\n",
    "\n",
    "\n",
    "### Agent and Environment\n",
    "\n",
    "Agent는 Environment(여기서는 Atari Game)과 연동을 합니다.<br>\n",
    "각각의 time-step마다 Agent는 $ A = {1, ..., K} $중의 하나의 액션 $ a_t $를 선택하고, Environment로부터 화면 이미지 $ x_t \\in \\mathbb{R}^d $와 reward $ r_t $를 받습니다.<br>\n",
    "하지만 하나의 게임 화면 이미지로만으로는 예를 들어 블록깨기에서 공이 어디 방향으로 가는지 알 수가 없습니다. <br>\n",
    "따라서 $ x_t $를 시계열의 데이터로 받으며 일련의 actions과 observations 의 연속성이 됩니다.\n",
    "\n",
    "$$ s_t = x_1, a_1, x_2, ..., a_{t-1}, x_t  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Discounted Return \n",
    "\n",
    "Agent의 목표는 future reward를 최대치로 한느 actions을 선택하는 것입니다.<br>\n",
    "여기서 future reward란 $ \\gamma $ 배수만큼 (a factor of $ \\gamma $ per time-step) discounted 되는 것을 의미합니다. <br>\n",
    "쉽게 이야기해서 먼미래의 reward일수록, 더 적은 reward로 계산하겠다는 뜻입니다.\n",
    "\n",
    "### $$ R_t = \\sum^T_{t^{\\prime} = t} \\gamma^{t^{\\prime} - t} r_{t^{\\prime}} $$\n",
    "\n",
    "$ T $는 게임이 끝나는 시점의 time-step을 의미합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Action-Value Function\n",
    "\n",
    "$$ \\begin{align}\n",
    "Q^{*} (s, a) &= \\max_{\\pi} \\mathbb{E} \\left[ r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + ... | s_t =s, a_t = a, \\pi \\right] \\\\\n",
    "&= \\max_{\\pi} \\mathbb{E} \\left[ R_t \\big| \\ s_t=s, a_t =a, \\pi \\right] \n",
    "\\end{align} $$\n",
    "\n",
    "여기서 $ \\pi = P(a|s) $는 behaviour policy로서 또는 actions에 대한 분포도를 나타냅니다.\n",
    "\n",
    "Optimal Action-Value Function은 Bellman Equation을 따릅니다. \n",
    "\n",
    "$$ Q^{*} (s, a) = E_{s^{\\prime} \\sim \\varepsilon} \\big[ r + \\gamma \\max_{a^{\\prime}} Q^{*} (s^{\\prime}, a^{\\prime}) \\  \\big| \\ s, a \\ \\big] $$\n",
    "\n",
    "기본적으로 많은 reinforcement learning algorithms들의 아이디어는 <br>\n",
    "바로 위의 action-value function을 iterative update같은 Bellman equation을 사용하여 측정하는 것입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay and Loss\n",
    "\n",
    "게임을 진행하면서 학습을 할 경우 observation sequence같의 연관성(correlation)때문에 학습이 제대로 안 될수 있습니다.<br>\n",
    "연관성을 끊어주는 방법으로 experience replay를 사용합니다.\n",
    "\n",
    "먼저 Agent의 experiences $ e_t = (s_t, a_t, r_t, s_{t+1} )$를 각각의 time-step마다 data set $ D_t = \\{ e_1, e_2, ..., e_t \\} $에 저장합니다.<br>\n",
    "학습시 Q-Learning updates를 uniformly random으로 꺼내진 experiences $ (s, a, r, s^{\\prime}) \\sim U(D) $ 통해 실행하게 됩니다.<br>\n",
    "Q-Learning update는 다음의 loss function을 사용하게 됩니다.\n",
    "\n",
    "$$ L_i(\\theta_i) = \\mathbb{E}_{s, a, r, s^{\\prime}} \\sim U(D) \\left[ \\left( r + \\gamma \\max_{a^{\\prime}} Q\\left(s^{\\prime}, a^{\\prime}; \\theta_{i-1}\\right) - Q\\left( s, a; \\theta_i \\right) \\right)^2 \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Learning with Experience Replay Algorithm\n",
    "\n",
    "<img src=\"./images/deep-q-learning-algorithm.png\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
