{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST, mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data\n",
    "\n",
    "* [Pytorch Transform Documentation](http://pytorch.org/docs/torchvision/transforms.html)\n",
    "\n",
    "\n",
    "1. **torchvision.transforms.Compose:** 여러개의 tranforms을 실행합니다. \n",
    "2. **torchvision.transforms.ToTensor:** PIL.Image 또는 [0, 255] range의 Numpy array(H x W x C)를 (C x H x W)의 **[0.0, 1.0] range**를 갖은 torch.FloatTensor로 변형시킵니다. <br>여기서 포인트가 0에서 1사이의 값을 갖은 값으로 normalization이 포함되있습니다. \n",
    "3. **dataloader.DataLoader:** 사용하여 training시킬때 1개의 batch를 가져올때 shape이 **torch.Size([64, 1, 28, 28])** 이렇게 나옵니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.utils.data.dataset' from '/usr/local/lib/python3.6/site-packages/torch/utils/data/dataset.py'>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = MNIST('./data', train=True, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "]), )\n",
    "\n",
    "test = MNIST('./data', train=False, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "]), )\n",
    "\n",
    "\n",
    "train_loader = dataloader.DataLoader(train, shuffle=True, batch_size=64, num_workers=1, pin_memory=True)\n",
    "test_loader = dataloader.DataLoader(test, shuffle=True, batch_size=64, num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.train_data = train.train_data.cuda()\n",
    "test.test_data = test.test_data.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train]\n",
      " - Numpy Shape: (60000, 28, 28)\n",
      " - Tensor Shape: torch.Size([60000, 28, 28])\n",
      " - Transformed Shape: torch.Size([28, 60000, 28])\n",
      " - min: 0.0\n",
      " - max: 1.0\n",
      " - mean: 0.13066047740240005\n",
      " - std: 0.3081078089011192\n",
      " - var: 0.0949304219058486\n"
     ]
    }
   ],
   "source": [
    "train_data = train.train_data\n",
    "train_data = train.transform(train_data.numpy())\n",
    "\n",
    "print('[Train]')\n",
    "print(' - Numpy Shape:', train.train_data.cpu().numpy().shape)\n",
    "print(' - Tensor Shape:', train.train_data.size())\n",
    "print(' - Transformed Shape:', train_data.size())\n",
    "print(' - min:', torch.min(train_data))\n",
    "print(' - max:', torch.max(train_data))\n",
    "print(' - mean:', torch.mean(train_data))\n",
    "print(' - std:', torch.std(train_data))\n",
    "print(' - var:', torch.var(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, parameters=[784, 625, 361, 144, 49, 10]):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(784, 625)\n",
    "        self.bc1 = nn.BatchNorm1d(625)\n",
    "        \n",
    "        self.fc2 = nn.Linear(625, 361)\n",
    "        self.bc2 = nn.BatchNorm1d(361)\n",
    "        \n",
    "        self.fc3 = nn.Linear(361, 144)\n",
    "        self.bc3 = nn.BatchNorm1d(144)\n",
    "        \n",
    "        self.fc4 = nn.Linear(144, 49)\n",
    "#         self.bc4 = nn.BatchNorm1d(49)\n",
    "        \n",
    "        self.fc5 = nn.Linear(49, 10)\n",
    "        \n",
    "        \n",
    "        self.fc6 = nn.Linear(784, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view((-1, 784))\n",
    "        h = self.fc1(x)\n",
    "        h = self.bc1(h)\n",
    "        h = nf.relu(h)\n",
    "#         h = F.dropout(h, p=0.2, training=self.training)\n",
    "        \n",
    "        h = self.fc2(h)\n",
    "        h = self.bc2(h)\n",
    "        h = nf.relu(h)\n",
    "        \n",
    "        h = self.fc3(h)\n",
    "        h = self.bc3(h)\n",
    "        h = nf.relu(h)\n",
    "        \n",
    "        h = self.fc4(h)\n",
    "        h = nf.relu(h)\n",
    "        \n",
    "        h = self.fc5(h)\n",
    "        out = nf.sigmoid(h)\n",
    "        return out\n",
    "\n",
    "model = Model()\n",
    "model.cuda() # CUDA!\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: [0/60000 (0%)]\tLoss: -0.999996\n",
      "Train Epoch: [6400/60000 (11%)]\tLoss: -0.999996\n",
      "Train Epoch: [12800/60000 (21%)]\tLoss: -0.999996\n",
      "Train Epoch: [19200/60000 (32%)]\tLoss: -0.999996\n",
      "Train Epoch: [25600/60000 (43%)]\tLoss: -0.999997\n",
      "Train Epoch: [32000/60000 (53%)]\tLoss: -0.999997\n",
      "Train Epoch: [38400/60000 (64%)]\tLoss: -0.999998\n",
      "Train Epoch: [44800/60000 (75%)]\tLoss: -0.999998\n",
      "Train Epoch: [51200/60000 (85%)]\tLoss: -0.999998\n",
      "Train Epoch: [57600/60000 (96%)]\tLoss: -0.999998\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(data) # Predict\n",
    "    \n",
    "    loss = F.nll_loss(y_pred, target) # Negative Log Likelihood Loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if batch_idx % 100 == 0:\n",
    "        print('Train Epoch: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
